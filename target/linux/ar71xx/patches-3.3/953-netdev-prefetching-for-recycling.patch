From 82d6698a1d2e1d261cb0c586970d8d331e88475b Mon Sep 17 00:00:00 2001
From: Ben Menchaca <ben.menchaca@qca.qualcomm.com>
Date: Tue, 11 Jun 2013 10:46:58 -0500
Subject: [netdev] prefetching for recycling

Added some simple prefetches during the recycle alloc so that the
subsequent writes to shinfo will not cause a data miss.  Also, added
a prefetch for skb->destructor that was another almost-certain miss.

Signed-off-by: Ben Menchaca <ben.menchaca@qca.qualcomm.com>
---
 net/core/skbuff.c | 45 ++++++++++++++++++++++++++++++++++++---------
 1 file changed, 36 insertions(+), 9 deletions(-)

diff --git a/net/core/skbuff.c b/net/core/skbuff.c
index 15b5f83..9336366 100644
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@ -373,16 +373,26 @@ struct sk_buff *__netdev_alloc_skb(struct net_device *dev,
 		skb = __skb_dequeue(h);
 		put_cpu_var(recycle_list);
 		if (likely(skb)) {
-			struct skb_shared_info *s = skb_shinfo(skb);
-			zero_struct(s,
-				    offsetof(struct skb_shared_info, dataref));
-			atomic_set(&s->dataref, 1);
+			struct skb_shared_info *shinfo;
+
+			/*
+			 * We're about to write a large amount to the skb to
+			 * zero most of the structure so prefetch the start
+			 * of the shinfo region now so it's in the D-cache
+			 * before we start to write that.
+			 */
+			prefetchw(&skb->end);
+
 			zero_struct(skb, offsetof(struct sk_buff, tail));
 			skb->data = skb->head;
 			skb_reset_tail_pointer(skb);
 #ifdef NET_SKBUFF_DATA_USES_OFFSET
 			skb->mac_header = ~0U;
 #endif
+			shinfo = skb_shinfo(skb);
+			zero_struct(shinfo, offsetof(struct skb_shared_info, dataref));
+			atomic_set(&shinfo->dataref, 1);
+
 			skb_reserve(skb, NET_SKB_PAD);
 			skb->dev = dev;
 			return skb;
@@ -443,28 +453,42 @@ struct sk_buff *dev_alloc_skb(unsigned int length)
 {
 	unsigned int len;
 
+	/*
+	 * Is this a request for an skb that we might be able to pull
+	 * from the recycling list?
+	 */
 	if (likely(length <= SKB_RECYCLE_SIZE)) {
 		struct sk_buff *skb;
 		struct sk_buff_head *h = &get_cpu_var(recycle_list);
 		skb = __skb_dequeue(h);
 		put_cpu_var(recycle_list);
 		if (likely(skb)) {
-			struct skb_shared_info *s = skb_shinfo(skb);
-			zero_struct(s,
-				    offsetof(struct skb_shared_info, dataref));
-			atomic_set(&s->dataref, 1);
+			struct skb_shared_info *shinfo;
+			/*
+			 * We're about to write a large amount to the skb to
+			 * zero most of the structure so prefetch the start
+			 * of the shinfo region now so it's in the D-cache
+			 * before we start to write that.
+			 */
+			prefetchw(&skb->end);
 			zero_struct(skb, offsetof(struct sk_buff, tail));
-			skb->data = skb->head + NET_SKB_PAD;
+			skb->data = skb->head;
 			skb_reset_tail_pointer(skb);
 			skb_reserve(skb, NET_SKB_PAD);
 #ifdef NET_SKBUFF_DATA_USES_OFFSET
 			skb->mac_header = ~0U;
 #endif
+			shinfo = skb_shinfo(skb);
+			zero_struct(shinfo,
+				    offsetof(struct skb_shared_info, dataref));
+			atomic_set(&shinfo->dataref, 1);
+
 			return skb;
 		}
 	}
 
 	len = (length <= SKB_RECYCLE_SIZE) ? SKB_RECYCLE_SIZE : len;
+
 	/*
 	 * There is more code here than it seems:
 	 * __dev_alloc_skb is an inline
@@ -671,6 +695,9 @@ void consume_skb(struct sk_buff *skb)
 {
 	if (unlikely(!skb))
 		return;
+
+	prefetch(&skb->destructor);
+
 	if (likely(atomic_read(&skb->users) == 1))
 		smp_rmb();
 	else if (likely(!atomic_dec_and_test(&skb->users)))
-- 
1.8.1.2

